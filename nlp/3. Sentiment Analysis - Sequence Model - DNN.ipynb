{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Imports and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/rishushrivastava/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rishushrivastava/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweet():\n",
    "    '''\n",
    "        Load the positive and negative tweets\n",
    "    '''\n",
    "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "    \n",
    "    return positive_tweets, negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tweets length: 5000\n",
      "Negative Tweets length: 5000\n"
     ]
    }
   ],
   "source": [
    "positive_tweets, negative_tweets = load_tweet()\n",
    "\n",
    "print(f'Positive Tweets length: {len(positive_tweets)}')\n",
    "print(f'Negative Tweets length: {len(negative_tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting the positive and negative tweets in 80:20 split\n",
    "\n",
    "def split_pos_neg_tweets(pos_tweets, neg_tweets, split=0.8):\n",
    "    '''\n",
    "        Splits the positive and negative tweets and returns training and val_test datasets\n",
    "    '''\n",
    "    \n",
    "    max_train_rows = int(len(pos_tweets) * split)\n",
    "    \n",
    "    print(f'Splitting the dataset in the ratio: {split}')\n",
    "    \n",
    "    train_pos = pos_tweets[:max_train_rows]\n",
    "    val_pos = pos_tweets[max_train_rows:]\n",
    "    \n",
    "    train_neg = neg_tweets[:max_train_rows]\n",
    "    val_neg = neg_tweets[max_train_rows:]\n",
    "    \n",
    "    train_label = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "    val_label = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "    \n",
    "    print(f'Total Training Rows (pos+neg) : {len(train_pos + train_neg)}')\n",
    "    print(f'Total Validation Rows (pos+neg): {len(val_pos + val_neg)}')\n",
    "    \n",
    "    return train_pos + train_neg , val_pos + val_neg, train_label, val_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataset in the ratio: 0.8\n",
      "Total Training Rows (pos+neg) : 8000\n",
      "Total Validation Rows (pos+neg): 2000\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, train_label, val_label = split_pos_neg_tweets(positive_tweets, negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data   : @stormieraae what the heck :( you don't follow her?\n",
      "Sample training label  : 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Sample training data   : {train_data[6000]}')\n",
    "print(f'Sample training label  : {train_label[6000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data  : #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Sample training label : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Sample training data  : {train_data[0]}')\n",
    "print(f'Sample training label : {train_label[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tweet_transform(tweets):\n",
    "    '''\n",
    "        Tokenize, remove stopword, remove hashtags and usernames, stem the words from tweets\n",
    "    '''\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    tweet = re.sub(r'#','',tweets)\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',tweet) ## remove any hyperlinks\n",
    "    tweet = re.sub(r'^RT[\\s]+','',tweet) ## remove any Retweets (RT)\n",
    "    \n",
    "    tokenizer = TweetTokenizer(preserve_case=True, reduce_len=False)\n",
    "    tweet_tokenise = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    cleaned_tweets = []\n",
    "    \n",
    "    for t in tweet_tokenise:\n",
    "        if t not in stop_words and t[0] != '@': ## ignore stopwords and usernames\n",
    "            stemmed_word = stemmer.stem(t) ## stem the words\n",
    "            cleaned_tweets.append(stemmed_word)\n",
    "    \n",
    "    return cleaned_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :) \n",
      "\n",
      "Transformed Tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(f'Original Tweet: \\n {train_data[0]} \\n')\n",
    "print(f'Transformed Tweet: \\n {tweet_transform(train_data[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet: \n",
      " @stormieraae what the heck :( you don't follow her? \n",
      "\n",
      "Transformed Tweet: \n",
      " ['heck', ':(', 'follow', '?']\n"
     ]
    }
   ],
   "source": [
    "print(f'Original Tweet: \\n {train_data[6000]} \\n')\n",
    "print(f'Transformed Tweet: \\n {tweet_transform(train_data[6000])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating word vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_vocab(tweets):\n",
    "    '''\n",
    "        The vocabulary of the tweet.\n",
    "    '''\n",
    "    \n",
    "    vocab = {'__PAD__':0, '__</e>__':1, '__UNK__':2}\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        \n",
    "        processed_tweet = tweet_transform(tweet)\n",
    "        \n",
    "        for word in processed_tweet:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary : 9422\n"
     ]
    }
   ],
   "source": [
    "vocab = tweet_vocab(train_data)\n",
    "\n",
    "print(f'Total vocabulary : {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\"data\":train_data,\"label\":train_label})\n",
    "#val_df = pd.DataFrame({\"data\":tweet_transform(val_data), \"label\":val_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2['transformed_data'] = train_df2['data'].apply(lambda x: tweet_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2['transformed_data'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Tensor + Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab, unknown_token = '__UNK__', verbose=False):\n",
    "    '''\n",
    "        Converts a tweet to tensors\n",
    "    '''\n",
    "    \n",
    "    tensor = []\n",
    "    processed_tweet = tweet_transform(tweet)\n",
    "    UNK_id = vocab.get(unknown_token)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'List of Processed Tweets')\n",
    "        print(processed_tweet)\n",
    "    \n",
    "    for word in processed_tweet:\n",
    "        tensor.append(vocab.get(word,UNK_id))\n",
    "        \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Tweet : \n",
      " Bro:U wan cut hair anot,ur hair long Liao bo\n",
      "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
      "Bro:LOL Sibei xialan\n",
      "Tensor: [1146, 204, 402, 527, 2478, 808, 8447, 72, 1208, 808, 62, 2, 2803, 1905, 204, 856, 2, 2, 72, 386, 654, 2, 3656, 1096, 650, 4761, 9, 1146, 204, 177, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(f'Actual Tweet : \\n {val_data[0]}')\n",
    "print(f'Tensor: {tweet_to_tensor(val_data[0],vocab,verbose=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2['transformed_data_tensor'] = train_df2['data'].apply(lambda x: tweet_to_tensor(x,vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_df2['transformed_data_tensor'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(pos_data, neg_data, vocab, batch_size=10, shuffle=False):\n",
    "    '''\n",
    "        Generates the processed tensor tweets\n",
    "        Inputs:\n",
    "            - pos_data : list of positive tweets\n",
    "            - neg_data : list of negative tweets\n",
    "            - vocab    : vocabulary generated above\n",
    "            - batch_size: number of items to be generated\n",
    "            - shuffle  : whether the items needs to be shuffled.\n",
    "        Output:\n",
    "            - generated output\n",
    "    '''\n",
    "    \n",
    "    len_pos_data = len(pos_data) ## total number of positive tweets\n",
    "    len_neg_data = len(neg_data) ## total number of negative tweets\n",
    "    \n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, labels, transform = None, train=True):\n",
    "        \n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.X_label = label[0]\n",
    "        self.y_label = label[1]\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[self.X_label]\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
