{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Logistic Classification for classifying tweets / text\n",
    "Given a tweet we will have to decide whether a tweet is positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/rishushrivastava/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rishushrivastava/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Analyse the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
       " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
       " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load positive tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "positive_tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopeless for tmr :(',\n",
       " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
       " '@Hegelbon That heart sliding into the waste basket. :(']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load negative tweets\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "negative_tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of Positive tweets: 5000\n",
      "Total No. of Negative tweets: 5000\n"
     ]
    }
   ],
   "source": [
    "## total number of pos and neg tweets\n",
    "\n",
    "print(f\"Total No. of Positive tweets: {len(positive_tweets)}\")\n",
    "print(f'Total No. of Negative tweets: {len(negative_tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data count train data: 8000 and test data : 2000\n"
     ]
    }
   ],
   "source": [
    "## generate a train and test dataset with equal combination of pos and neg tweets\n",
    "##Â in total 1000 words, dividing the list of tweets into 8000 train and 2000 test datasets.\n",
    "\n",
    "train_pos = positive_tweets[:4000]\n",
    "train_neg = negative_tweets[:4000]\n",
    "\n",
    "test_pos = positive_tweets[4000:]\n",
    "test_neg = negative_tweets[4000:]\n",
    "\n",
    "# combining all of them together\n",
    "\n",
    "train_data = train_pos + train_neg\n",
    "test_data = test_pos + test_neg\n",
    "\n",
    "print(f'Total number of data count train data: {len(train_data)} and test data : {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train and Test labels : (8000, 1) and (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# creating labels for the datasets\n",
    "train_label = np.append(np.ones((len(train_pos),1)), np.zeros((len(train_neg),1)), axis=0)\n",
    "test_label = np.append(np.ones((len(test_pos),1)), np.zeros((len(test_neg),1)), axis=0)\n",
    "\n",
    "print(f'Shape of Train and Test labels : {train_label.shape} and {test_label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of the data to create word frequencies list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "        clean the tweet to tokenise, remove stop words and stem the words\n",
    "    '''\n",
    "    stop_words = stopwords.words('english')\n",
    "    #print(f'Total stop words in the vocab: {len(stop_words)}')\n",
    "    \n",
    "    tweet = re.sub(r'#','',tweet) ## remove the # symbol\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',tweet) ## remove any hyperlinks\n",
    "    tweet = re.sub(r'^RT[\\s]+','',tweet) ## remove any Retweets (RT)\n",
    "    \n",
    "    tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_token = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweet_cleaned = []\n",
    "    \n",
    "    for word in tweet_token:\n",
    "        if word not in stop_words:\n",
    "            tweet_cleaned.append(word)\n",
    "            \n",
    "    return tweet_cleaned\n",
    "    \n",
    "\n",
    "def build_tweet_frequency(tweets, label):\n",
    "    '''\n",
    "        Build a vocab of tweet word frequencies across corpus. \n",
    "        @input: Tweets - list of tweets\n",
    "                label - Array of tweet sentiments\n",
    "        @output: a dict of (word, label):frequency\n",
    "    '''\n",
    "    label_list = np.squeeze(label).tolist()\n",
    "    \n",
    "    freq = {}\n",
    "    \n",
    "    for t, l in zip(tweets, label_list):\n",
    "        for word in clean_tweet(t):\n",
    "            word_pair = (word,l)\n",
    "            \n",
    "            if word_pair in freq:\n",
    "                freq[word_pair] +=1\n",
    "            else:\n",
    "                freq[word_pair] =1\n",
    "\n",
    "    return freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0] ## 0, 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['followfriday', 'top', 'engaged', 'members', 'community', 'week', ':)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweet(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_freq_vocab = build_tweet_frequency(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_freq_vocab.get(('sad',0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, vocab):\n",
    "    '''\n",
    "        Given a tweet and frequency vocab, generate a list of \n",
    "        @input: \n",
    "            tweet - tweet we want to extract features from\n",
    "            vocab - frequency vocab dictionary\n",
    "        @output:\n",
    "            tweet_feature - a numpy array with [label, total_pos_freq, total_neg_freq]\n",
    "    '''\n",
    "    cleaned_tweet = clean_tweet(tweet)\n",
    "    #print(cleaned_tweet)\n",
    "    tweet_feature = np.zeros((1,3))\n",
    "    \n",
    "    tweet_feature[0,0] = 1\n",
    "    \n",
    "    for words in cleaned_tweet: # iterate over the tweet to get the number of pos and neg tweet freqs\n",
    "        #print(vocab.get((words,1.0),0), \" --- \", vocab.get((words,0.0),0))\n",
    "        tweet_feature[0,1] += vocab.get((words,1.0),0)\n",
    "        tweet_feature[0,2] += vocab.get((words,0.0),0)\n",
    "    \n",
    "    return tweet_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 3.003e+03, 4.900e+01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(train_data[0],tweet_freq_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 805., 621.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features('Hi How are you? I am doing good', tweet_freq_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the vector word frequency for all of the training tweets\n",
    "\n",
    "train_X = np.zeros((len(train_data),3))\n",
    "for i in range(len(train_data)):\n",
    "    train_X[i,:] = extract_features(train_data[i], tweet_freq_vocab)\n",
    "\n",
    "train_y = train_label\n",
    "\n",
    "test_X = np.zeros((len(test_data),3))\n",
    "for i in range(len(test_data)):\n",
    "    test_X[i,:] = extract_features(test_data[i], tweet_freq_vocab)\n",
    "    \n",
    "test_y = test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 3.003e+03, 4.900e+01],\n",
       "       [1.000e+00, 6.280e+03, 1.724e+03],\n",
       "       [1.000e+00, 5.906e+03, 2.168e+03],\n",
       "       [1.000e+00, 2.862e+03, 4.000e+00],\n",
       "       [1.000e+00, 7.326e+03, 2.127e+03]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishushrivastava/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      1000\n",
      "         1.0       0.99      0.99      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making your own predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet1 = 'i liked my prediction score. happy with the results'\n",
    "model.predict(extract_features(my_tweet1,tweet_freq_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet2 = 'i am sad with the result of the football match'\n",
    "model.predict(extract_features(my_tweet2,tweet_freq_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet3 = 'shame that i couldnt get an entry to the competition'\n",
    "model.predict(extract_features(my_tweet3,tweet_freq_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet3 = 'this movie should have been great.'\n",
    "model.predict(extract_features(my_tweet3,tweet_freq_vocab)) ## misclassified example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet3 = 'i liked my prediction score. not happy with the results'\n",
    "model.predict(extract_features(my_tweet3,tweet_freq_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda5939810c181249b5bb2a2b326f0382de"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
