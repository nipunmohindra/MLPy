{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Naive Bayes to predict tweet sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/rishushrivastava/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rishushrivastava/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Analyse your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
       " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
       " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load positive tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "positive_tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopeless for tmr :(',\n",
       " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
       " '@Hegelbon That heart sliding into the waste basket. :(']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load negative tweets\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "negative_tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of Positive tweets: 5000\n",
      "Total No. of Negative tweets: 5000\n"
     ]
    }
   ],
   "source": [
    "## total number of pos and neg tweets\n",
    "\n",
    "print(f\"Total No. of Positive tweets: {len(positive_tweets)}\")\n",
    "print(f'Total No. of Negative tweets: {len(negative_tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data count train data: 8000 and test data : 2000\n"
     ]
    }
   ],
   "source": [
    "## generate a train and test dataset with equal combination of pos and neg tweets\n",
    "##Â in total 1000 words, dividing the list of tweets into 8000 train and 2000 test datasets.\n",
    "\n",
    "train_pos = positive_tweets[:4000]\n",
    "train_neg = negative_tweets[:4000]\n",
    "\n",
    "test_pos = positive_tweets[4000:]\n",
    "test_neg = negative_tweets[4000:]\n",
    "\n",
    "# combining all of them together\n",
    "\n",
    "train_data = train_pos + train_neg\n",
    "test_data = test_pos + test_neg\n",
    "\n",
    "print(f'Total number of data count train data: {len(train_data)} and test data : {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train and Test labels : (8000, 1) and (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# creating labels for the datasets\n",
    "train_label = np.append(np.ones((len(train_pos),1)), np.zeros((len(train_neg),1)), axis=0)\n",
    "test_label = np.append(np.ones((len(test_pos),1)), np.zeros((len(test_neg),1)), axis=0)\n",
    "\n",
    "print(f'Shape of Train and Test labels : {train_label.shape} and {test_label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of the words to create word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "        clean the tweet to tokenise, remove stop words and stem the words\n",
    "    '''\n",
    "    stop_words = stopwords.words('english')\n",
    "    #print(f'Total stop words in the vocab: {len(stop_words)}')\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tweet = re.sub(r'#','',tweet) ## remove the # symbol\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',tweet) ## remove any hyperlinks\n",
    "    tweet = re.sub(r'^RT[\\s]+','',tweet) ## remove any Retweets (RT)\n",
    "    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_token = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweet_cleaned = []\n",
    "    \n",
    "    for word in tweet_token:\n",
    "        if word not in stop_words:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            tweet_cleaned.append(stemmed_word)\n",
    "            \n",
    "    return tweet_cleaned\n",
    "    \n",
    "\n",
    "def build_tweet_frequency(tweets, label):\n",
    "    '''\n",
    "        Build a vocab of tweet word frequencies across corpus. \n",
    "        @input: Tweets - list of tweets\n",
    "                label - Array of tweet sentiments\n",
    "        @output: a dict of (word, label):frequency\n",
    "    '''\n",
    "    label_list = np.squeeze(label).tolist()\n",
    "    \n",
    "    freq = {}\n",
    "    \n",
    "    for t, l in zip(tweets, label_list):\n",
    "        for word in clean_tweet(t):\n",
    "            word_pair = (word,l)\n",
    "            \n",
    "            if word_pair in freq:\n",
    "                freq[word_pair] +=1\n",
    "            else:\n",
    "                freq[word_pair] =1\n",
    "\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_freq_vocab = build_tweet_frequency(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_freq_vocab.get(('happi',1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing some tweets\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "tweets_y = [1,0,0,0,0]\n",
    "\n",
    "build_tweet_frequency(tweets, tweets_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayes1](images/bayes1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![naive bayes2](images/naivebayes2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logprior and loglikelihood](images/logprior_and_loglikelhood.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_model(x,y,vocab):\n",
    "    '''\n",
    "        @input:\n",
    "            - x: input a list of train data\n",
    "            - y: input a list of labels\n",
    "            - vocab: the frequency vocab\n",
    "        @output:\n",
    "            - loglikelihood: \n",
    "            - logprior\n",
    "    '''\n",
    "    \n",
    "    logprior = 0\n",
    "    loglikelihood = {}\n",
    "    \n",
    "    #total number of distinct words in the vocab. Remember words can be in both neg and pos\n",
    "    V = len(set([items[0] for items in vocab])) \n",
    "    \n",
    "    N_pos = N_neg = V_pos = V_neg = 0\n",
    "    for pair in vocab.keys():\n",
    "        if pair[1] > 0:\n",
    "            V_pos += 1 # increment the count of unique positive words by 1\n",
    "            N_pos += vocab[pair] # Increment the number of positive words by the count for this (word, label) pair\n",
    "            \n",
    "        else:    \n",
    "            V_neg += 1 # increment the count of unique negative words by 1\n",
    "            N_neg += vocab[pair] # increment the number of negative words by the count for this (word,label) pair\n",
    "    \n",
    "    # the number of documents\n",
    "    D = len(y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11406"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_freq_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9120"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([items[0] for items in tweet_freq_vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda5939810c181249b5bb2a2b326f0382de"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
